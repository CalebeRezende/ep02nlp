# EP02- 
Calebe Macena Rezende
Nº USP: 9366151
# EP02: Projeto de Regressão e Quantização de Transformadores

## Visão geral
O objetivo deste artigo é apresentar  o refinamento de encoders na arquitetura Transformers (BERT-like, usando o:  refinamento (finetuning) da rede neural BERTimbau, Com uma tarefa de regressão usando uma variação do BERT, prevemos a densidade de vogais em cada sentença, esse cálculo foi feito  em tarefas balanceadas e desbalanceadas 


# Requisitos
Para usar este (.ipynb), foi necessário usar A100 do Colab, para carregar, sem o mesmo há a desconexão por inatividade.

Para rodar todos os cálculos, pode ser feito via: execute.tudo, não há qualquer coisa que precise ser indexado. 

No mais, desfrute lendo o artigo desenvolvido a partir desse git.

Para rodar tudo, basta usar Python 3+, e o córpus pode ser alterado a critério de qual sentença deseja avaliar a densidade de vogais.
